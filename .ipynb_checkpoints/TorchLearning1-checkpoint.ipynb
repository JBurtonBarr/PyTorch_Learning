{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#torch will be reliant on the np datastructure for tensors\n",
    "\n",
    "#We want to initialise our tensor by offering data to a numpy array and then parsing that array into torch\n",
    "\n",
    "data = [[1, 2, 3],[3, 4, 5], [2, 2, 1],[5, 2, 3]]\n",
    "#we could do torch_data = torch.tensor(data) this is more direct\n",
    "# more often that not though our data will be in a numpy array\n",
    "\n",
    "np_a = np.array(data)\n",
    "tensor_np = torch.from_numpy(np_a) #good\n",
    "\n",
    "\n",
    "#we can make a shape and datatype like tensor from another tensor\n",
    "datashapeclone = torch.ones_like(tensor_np) #will be filled with 1's\n",
    "#random clone \n",
    "datashapeclone_rand = torch.rand_like(tensor_np, dtype = torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7893, 0.8591, 0.3580],\n",
      "        [0.8229, 0.5127, 0.5114],\n",
      "        [0.0465, 0.6060, 0.1366],\n",
      "        [0.9622, 0.5497, 0.7792]])\n",
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [2, 2, 1],\n",
      "        [5, 2, 3]], dtype=torch.int32)\n",
      "Shape of tensor: torch.Size([4, 3])\n",
      "Datatype of tensor: torch.int32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "print(datashapeclone_rand)\n",
    "print(tensor_np)\n",
    "#the chape of these tensors are all 4 by 3\n",
    "#so the tensor is a shaped object that holds our data for ML\n",
    "print(f\"Shape of tensor: {tensor_np.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor_np.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor_np.device}\") #this is just getting tensor info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# we want to check that our GPU is available for our tensor operations \n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "  tensor_np = tensor_np.to('cuda')\n",
    "  #This will check if cuda is available and then move our tensor to cuda if so\n",
    "  # avoids a running error when cuda is not available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device tensor is stored on: {tensor_np.device}\")\n",
    "#Our tensor should now be stored on Cuda meaning GPU is accessable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 1, 2, 3],\n",
      "        [3, 4, 5, 3, 4, 5],\n",
      "        [2, 2, 1, 2, 2, 1],\n",
      "        [5, 2, 3, 5, 2, 3]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor_np, tensor_np], dim=1)\n",
    "print(t1)\n",
    "#notice how this will combine tensors along one dimension\n",
    "#we can also just STACK these "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Notes\n",
    "Tensors can be multiplied (look up when needed)\n",
    "We can also calculate matrix multiplication if needed (tensor.matmul(tensor.T2))\n",
    "\n",
    "We can convert our tensor back to nump with T1.numpy()\n",
    "\n",
    "Remember from_numpy is numpy to tensor\n",
    "\n",
    "Changes in the NumPy array reflects in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 1, 2, 3],\n",
      "        [3, 4, 5, 3, 4, 5],\n",
      "        [2, 2, 1, 2, 2, 1],\n",
      "        [5, 2, 3, 5, 2, 3]], device='cuda:0', dtype=torch.int32) \n",
      "\n",
      "tensor([[4, 5, 6, 4, 5, 6],\n",
      "        [6, 7, 8, 6, 7, 8],\n",
      "        [5, 5, 4, 5, 5, 4],\n",
      "        [8, 5, 6, 8, 5, 6]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(t1, \"\\n\")\n",
    "t1.add_(3)\n",
    "print(t1)\n",
    "#This will add 3 TO EVERY DATAPOINT WITHIN TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH.AUTOGRAD INTRO\n",
    "torch.autograd is PyTorchâ€™s automatic differentiation engine that powers neural network training\n",
    "\n",
    "### Forward Propagation: \n",
    "In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "### Backward Propagation: \n",
    "In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. For a more detailed walkthrough of backprop\n",
    "    - Where a huge difference from the human brain occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\JBurt/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c2f2872d554c73acf25c2afad4a6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46830571.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True) \n",
    "#our moodel is one of torchvisions pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1375, 0.8334, 0.6301,  ..., 0.0097, 0.6700, 0.6591],\n",
      "          [0.0031, 0.8220, 0.5331,  ..., 0.9853, 0.5732, 0.1289],\n",
      "          [0.4738, 0.6685, 0.3730,  ..., 0.3454, 0.7879, 0.3650],\n",
      "          ...,\n",
      "          [0.7513, 0.7537, 0.2620,  ..., 0.4592, 0.1149, 0.9920],\n",
      "          [0.1460, 0.4343, 0.5432,  ..., 0.9122, 0.5270, 0.4105],\n",
      "          [0.7718, 0.3579, 0.5132,  ..., 0.7121, 0.8172, 0.9797]],\n",
      "\n",
      "         [[0.4560, 0.0041, 0.9183,  ..., 0.7112, 0.2756, 0.8531],\n",
      "          [0.5088, 0.4783, 0.8168,  ..., 0.4907, 0.3000, 0.7401],\n",
      "          [0.5566, 0.6342, 0.1605,  ..., 0.8937, 0.8924, 0.3668],\n",
      "          ...,\n",
      "          [0.3136, 0.0437, 0.7279,  ..., 0.5918, 0.3352, 0.3405],\n",
      "          [0.7277, 0.1131, 0.9252,  ..., 0.3631, 0.8966, 0.4337],\n",
      "          [0.0787, 0.3264, 0.5362,  ..., 0.3474, 0.9628, 0.8277]],\n",
      "\n",
      "         [[0.4483, 0.6929, 0.4825,  ..., 0.1628, 0.1497, 0.7907],\n",
      "          [0.3054, 0.5254, 0.8907,  ..., 0.6145, 0.6125, 0.0656],\n",
      "          [0.8065, 0.7280, 0.1156,  ..., 0.0839, 0.7604, 0.2389],\n",
      "          ...,\n",
      "          [0.8700, 0.4144, 0.7809,  ..., 0.5084, 0.7196, 0.5054],\n",
      "          [0.7725, 0.4447, 0.4134,  ..., 0.7769, 0.4482, 0.6611],\n",
      "          [0.0211, 0.4382, 0.6330,  ..., 0.5014, 0.5598, 0.5584]]]])\n",
      "============================================ \n",
      " \n",
      "\n",
      "tensor([[7.8867e-01, 5.2074e-01, 2.9322e-01, 1.5684e-01, 8.8057e-01, 3.9042e-02,\n",
      "         7.6010e-01, 6.1387e-01, 6.9973e-01, 6.9702e-01, 2.1313e-01, 8.8493e-01,\n",
      "         4.6431e-01, 9.6933e-02, 8.5823e-01, 5.8539e-01, 2.3402e-01, 1.7801e-01,\n",
      "         5.0597e-01, 7.5200e-01, 8.5227e-01, 7.1879e-02, 7.5193e-01, 9.6899e-02,\n",
      "         5.3746e-02, 6.1658e-01, 3.8312e-02, 2.0876e-01, 8.3258e-01, 7.7227e-01,\n",
      "         8.2370e-01, 9.7390e-02, 1.6596e-01, 9.2919e-02, 1.5412e-01, 1.2477e-01,\n",
      "         7.0523e-01, 2.5864e-01, 6.4335e-01, 2.5165e-01, 8.1547e-01, 6.5678e-01,\n",
      "         3.3355e-01, 8.4192e-01, 9.7032e-01, 8.1050e-01, 1.2707e-01, 1.1004e-01,\n",
      "         3.2497e-01, 4.6259e-01, 1.1243e-01, 6.6423e-01, 9.7166e-03, 7.6495e-02,\n",
      "         7.6567e-01, 1.9805e-01, 5.8697e-01, 9.7732e-01, 6.6278e-01, 9.0296e-01,\n",
      "         9.6945e-01, 1.9492e-01, 7.1655e-01, 2.2657e-01, 4.8346e-01, 3.3796e-01,\n",
      "         4.0779e-01, 8.2307e-01, 3.7966e-01, 8.5372e-01, 2.5518e-01, 7.4973e-01,\n",
      "         5.1757e-01, 3.5187e-01, 5.1929e-01, 2.3587e-01, 7.4092e-01, 1.7918e-01,\n",
      "         5.3254e-01, 4.8177e-01, 1.4367e-02, 5.8267e-01, 5.0859e-01, 3.5626e-01,\n",
      "         5.1669e-01, 7.6634e-01, 6.9895e-01, 1.5634e-02, 2.9907e-01, 6.4997e-01,\n",
      "         2.3961e-01, 5.6259e-01, 1.4428e-01, 4.9929e-01, 8.5423e-01, 6.6277e-01,\n",
      "         1.2032e-01, 5.8859e-01, 4.0079e-01, 4.6420e-01, 5.3159e-01, 6.6889e-01,\n",
      "         6.9899e-01, 5.3361e-01, 9.2893e-01, 4.7746e-02, 9.0987e-01, 9.4231e-01,\n",
      "         2.5620e-01, 4.4661e-01, 7.6138e-01, 5.9628e-02, 8.1927e-01, 9.6249e-03,\n",
      "         7.7040e-01, 5.5742e-02, 8.1920e-01, 3.3969e-01, 4.8794e-01, 2.9664e-02,\n",
      "         5.7672e-01, 9.1223e-01, 5.2836e-01, 9.4448e-01, 3.5078e-02, 6.4559e-01,\n",
      "         8.4948e-01, 2.6909e-01, 8.7686e-01, 8.3940e-01, 2.5991e-01, 8.2070e-01,\n",
      "         4.5927e-01, 6.4591e-03, 7.9175e-01, 6.0942e-01, 8.4394e-01, 6.5969e-01,\n",
      "         1.0066e-02, 6.3055e-01, 5.4458e-01, 3.4587e-01, 8.0473e-01, 2.9035e-01,\n",
      "         1.1772e-01, 1.0814e-02, 5.8191e-01, 5.2529e-01, 1.3563e-01, 7.5103e-01,\n",
      "         5.7443e-01, 3.0725e-01, 2.8194e-01, 1.8502e-01, 1.1577e-01, 3.0548e-01,\n",
      "         5.1642e-01, 5.3871e-01, 1.4283e-02, 9.8700e-01, 7.0197e-01, 4.5198e-01,\n",
      "         6.2597e-01, 7.5423e-01, 5.7664e-01, 2.9758e-01, 9.3707e-02, 2.8866e-01,\n",
      "         1.7263e-01, 9.2625e-01, 5.3422e-01, 3.2603e-01, 1.0069e-01, 2.6026e-01,\n",
      "         1.3401e-01, 6.5975e-01, 6.0379e-01, 6.8132e-01, 1.3101e-01, 6.4626e-03,\n",
      "         6.9201e-01, 6.9963e-01, 5.7100e-02, 7.7343e-01, 2.5130e-01, 8.9453e-01,\n",
      "         3.7747e-02, 3.0102e-01, 5.4775e-01, 5.6486e-01, 4.9108e-01, 5.7902e-01,\n",
      "         5.9144e-01, 7.5769e-01, 9.0111e-01, 4.3409e-01, 4.2449e-01, 6.4321e-01,\n",
      "         3.6464e-01, 6.1847e-01, 3.4293e-01, 8.6745e-01, 1.6064e-01, 8.4234e-01,\n",
      "         3.7531e-01, 9.7234e-01, 3.2544e-01, 8.6212e-01, 1.5860e-01, 9.4342e-04,\n",
      "         6.8091e-01, 6.8223e-01, 8.1797e-01, 5.1565e-01, 9.1368e-01, 1.4817e-01,\n",
      "         2.8863e-01, 4.0609e-01, 7.1184e-01, 4.5200e-01, 8.5417e-01, 4.2277e-01,\n",
      "         3.9814e-01, 5.2800e-01, 3.5207e-01, 9.1269e-01, 3.0819e-01, 6.9730e-01,\n",
      "         4.8206e-01, 1.7456e-01, 7.2855e-01, 3.3286e-01, 7.8078e-01, 4.7881e-01,\n",
      "         6.3311e-01, 3.6642e-01, 3.6140e-01, 8.5099e-01, 4.8361e-01, 7.6726e-01,\n",
      "         7.9371e-03, 8.5762e-01, 9.9272e-01, 8.6820e-01, 3.2111e-01, 8.3962e-01,\n",
      "         1.5606e-01, 8.5779e-01, 8.3864e-02, 8.7490e-01, 2.0337e-01, 1.4946e-01,\n",
      "         4.9919e-01, 3.6649e-01, 9.9204e-01, 3.3684e-01, 8.0040e-01, 7.8742e-01,\n",
      "         6.2473e-01, 8.4070e-01, 5.8338e-01, 3.0438e-01, 9.3140e-01, 7.5213e-02,\n",
      "         3.5794e-01, 4.5913e-01, 7.1697e-01, 7.2830e-01, 9.0980e-01, 1.4437e-01,\n",
      "         8.5813e-01, 9.8044e-01, 8.4621e-01, 8.0520e-02, 2.8239e-01, 9.6392e-01,\n",
      "         5.0982e-01, 9.7602e-01, 2.0281e-01, 2.7299e-01, 5.2434e-02, 9.8501e-01,\n",
      "         6.8595e-01, 9.2640e-01, 3.4044e-01, 8.7949e-01, 1.6764e-01, 1.7525e-01,\n",
      "         3.3186e-01, 4.6009e-01, 4.5922e-01, 2.9889e-03, 1.4372e-01, 6.2992e-01,\n",
      "         5.3890e-01, 6.7246e-01, 7.8192e-01, 1.1060e-01, 4.6611e-01, 9.7465e-01,\n",
      "         3.0113e-02, 8.1839e-01, 8.4031e-01, 9.5122e-01, 8.4020e-01, 8.0128e-01,\n",
      "         8.6747e-01, 6.2990e-01, 1.3863e-01, 1.7188e-01, 6.4030e-01, 3.9981e-01,\n",
      "         4.7204e-01, 6.9136e-01, 7.0092e-01, 3.6684e-01, 7.1171e-01, 2.9336e-01,\n",
      "         4.1485e-01, 3.9748e-02, 5.9886e-01, 7.6628e-01, 3.1788e-01, 1.9465e-01,\n",
      "         7.8042e-01, 4.9702e-01, 9.8960e-01, 3.1143e-01, 4.6747e-02, 7.1822e-01,\n",
      "         4.2265e-01, 6.9250e-01, 3.8124e-01, 7.3023e-01, 8.2113e-01, 4.0577e-01,\n",
      "         3.0692e-01, 3.8600e-01, 3.8467e-01, 5.4974e-01, 9.8709e-02, 9.4736e-01,\n",
      "         7.7965e-02, 5.7967e-01, 8.9618e-02, 7.8332e-01, 7.9385e-02, 7.8844e-01,\n",
      "         6.4251e-01, 3.6644e-02, 3.6283e-01, 3.2352e-01, 7.5034e-01, 3.9748e-01,\n",
      "         6.5591e-01, 8.3253e-01, 8.1628e-01, 1.2431e-02, 5.7632e-01, 6.1536e-02,\n",
      "         8.5992e-01, 8.9787e-01, 8.4835e-01, 5.5086e-01, 3.9944e-01, 6.9737e-01,\n",
      "         2.6490e-01, 6.6525e-02, 8.0631e-01, 9.2556e-01, 7.8477e-01, 1.9219e-01,\n",
      "         6.7863e-01, 7.5129e-01, 5.3403e-01, 7.2739e-01, 7.4887e-02, 8.3003e-01,\n",
      "         1.8384e-01, 9.9946e-01, 7.4607e-01, 5.3038e-01, 9.5478e-01, 3.1738e-01,\n",
      "         5.8804e-01, 4.9278e-01, 8.9259e-01, 5.7102e-01, 4.2607e-01, 3.0900e-01,\n",
      "         7.6690e-01, 3.3232e-01, 5.2401e-01, 6.2492e-01, 5.4751e-01, 4.1075e-01,\n",
      "         4.9754e-01, 9.1511e-02, 1.3056e-01, 2.8803e-01, 3.8108e-01, 2.6561e-02,\n",
      "         3.6009e-01, 9.7351e-02, 3.5949e-01, 8.9883e-03, 5.5765e-01, 9.4060e-01,\n",
      "         2.8759e-02, 8.0019e-03, 2.9232e-01, 7.1112e-01, 5.1344e-02, 9.6560e-01,\n",
      "         7.1821e-01, 1.7899e-01, 1.8333e-01, 2.5186e-01, 6.1882e-01, 4.7419e-01,\n",
      "         5.2452e-01, 7.6809e-01, 8.3691e-01, 3.7088e-01, 9.5117e-01, 8.4817e-01,\n",
      "         6.4860e-01, 3.6415e-01, 1.2271e-01, 9.9292e-01, 5.6210e-01, 2.4363e-01,\n",
      "         8.2111e-01, 3.2457e-02, 4.4759e-01, 1.1714e-01, 5.3609e-01, 4.6606e-01,\n",
      "         3.6590e-01, 4.7537e-01, 6.7500e-01, 3.7875e-01, 5.2266e-01, 4.0511e-01,\n",
      "         9.8585e-01, 3.8500e-01, 8.0868e-01, 9.6760e-01, 5.9415e-02, 2.5664e-01,\n",
      "         2.0909e-01, 4.5761e-02, 7.2698e-01, 2.6948e-01, 9.6929e-01, 8.4224e-01,\n",
      "         7.9069e-02, 3.1756e-01, 6.3161e-01, 1.7996e-01, 2.9323e-01, 7.3904e-01,\n",
      "         4.7779e-01, 3.9537e-02, 2.6960e-01, 3.6824e-01, 6.2009e-01, 4.4062e-01,\n",
      "         7.5103e-01, 9.3329e-02, 1.4732e-01, 4.2411e-01, 3.0027e-01, 8.6677e-01,\n",
      "         7.9250e-01, 8.7216e-01, 8.9686e-01, 1.8987e-02, 7.5617e-01, 1.3402e-01,\n",
      "         3.6707e-01, 9.1843e-01, 2.6227e-01, 9.3616e-01, 5.5604e-01, 4.9630e-01,\n",
      "         7.1992e-01, 4.4039e-01, 8.3375e-01, 2.7452e-01, 9.0635e-01, 4.1194e-01,\n",
      "         2.9547e-02, 3.3528e-01, 4.4427e-01, 7.6889e-01, 1.4906e-01, 1.6490e-01,\n",
      "         6.7720e-01, 2.6549e-01, 8.5285e-01, 2.2563e-01, 6.4791e-01, 9.9093e-01,\n",
      "         3.6404e-02, 3.6980e-01, 1.0658e-01, 7.9559e-01, 4.9015e-01, 7.1819e-01,\n",
      "         7.9713e-01, 4.2321e-01, 9.9683e-01, 3.2825e-01, 4.9851e-02, 8.9190e-01,\n",
      "         1.1636e-01, 9.2912e-01, 9.7694e-02, 9.4435e-01, 1.1270e-01, 5.5913e-02,\n",
      "         6.0378e-01, 4.8739e-01, 8.2354e-01, 1.2106e-01, 8.7624e-01, 5.7383e-01,\n",
      "         6.0632e-01, 9.5217e-01, 9.2388e-01, 7.3502e-01, 3.5866e-01, 9.7724e-01,\n",
      "         3.0259e-01, 8.6659e-01, 8.7151e-01, 9.5676e-01, 6.4059e-01, 9.3886e-02,\n",
      "         2.9482e-01, 4.2926e-01, 8.0365e-01, 5.5283e-01, 4.8866e-01, 6.5279e-01,\n",
      "         9.2655e-02, 4.6920e-01, 2.3381e-01, 8.5691e-01, 7.5577e-01, 4.5427e-01,\n",
      "         8.3800e-01, 5.5403e-01, 8.6282e-02, 1.3494e-01, 5.0564e-01, 1.3084e-01,\n",
      "         1.9132e-02, 3.3476e-01, 7.4489e-01, 2.1399e-01, 5.9689e-01, 9.5847e-01,\n",
      "         5.7786e-01, 8.8255e-01, 8.7428e-01, 5.0282e-01, 8.2160e-01, 9.1614e-01,\n",
      "         8.9037e-01, 8.1121e-01, 1.9771e-01, 4.5278e-02, 5.9122e-01, 5.3617e-01,\n",
      "         1.6378e-01, 6.3560e-01, 7.2378e-01, 5.7797e-01, 4.0238e-01, 2.7962e-01,\n",
      "         4.6172e-01, 5.9910e-01, 2.0268e-01, 3.0659e-01, 2.3931e-01, 7.2832e-01,\n",
      "         5.8162e-01, 1.7332e-01, 9.4049e-01, 1.2139e-01, 2.1672e-01, 1.0233e-01,\n",
      "         8.3412e-01, 7.4296e-02, 6.5832e-01, 2.7186e-01, 6.9912e-01, 2.9429e-01,\n",
      "         6.3194e-01, 2.9088e-01, 3.5676e-01, 4.4824e-01, 3.3833e-02, 8.3074e-01,\n",
      "         8.9141e-02, 8.4842e-01, 6.5054e-01, 5.6096e-03, 4.5803e-01, 8.3093e-01,\n",
      "         9.3452e-01, 6.5589e-01, 3.8018e-01, 3.5373e-01, 5.7012e-01, 1.9551e-01,\n",
      "         4.2662e-01, 9.8763e-01, 6.3058e-01, 9.2759e-03, 7.4690e-01, 9.2896e-01,\n",
      "         4.7888e-01, 3.5345e-01, 6.5091e-01, 5.9429e-01, 7.8106e-01, 9.4087e-01,\n",
      "         2.2671e-01, 3.3812e-01, 6.3251e-01, 6.7362e-01, 7.9711e-01, 8.6949e-01,\n",
      "         3.6417e-02, 5.3670e-01, 7.8871e-01, 7.6220e-01, 4.4183e-01, 1.0082e-01,\n",
      "         2.3151e-01, 5.8003e-01, 1.7787e-01, 7.5403e-01, 5.4599e-01, 1.7583e-01,\n",
      "         7.2505e-01, 6.0237e-01, 7.2978e-01, 2.0682e-01, 9.8704e-01, 4.0269e-01,\n",
      "         7.7597e-01, 4.2222e-02, 4.6475e-01, 9.3645e-01, 4.7873e-01, 9.4529e-01,\n",
      "         5.7887e-01, 1.4781e-01, 1.1213e-01, 4.2305e-01, 6.7940e-01, 5.5787e-01,\n",
      "         1.2879e-02, 4.9627e-01, 3.2291e-01, 4.4105e-01, 4.3920e-01, 5.3336e-01,\n",
      "         3.1947e-01, 1.6501e-01, 7.4366e-01, 1.7168e-01, 9.0079e-01, 4.8642e-01,\n",
      "         2.9101e-01, 8.9629e-01, 7.2955e-01, 1.9229e-01, 9.5346e-01, 4.1552e-01,\n",
      "         4.0546e-01, 2.4453e-02, 9.7473e-01, 4.9876e-01, 8.3240e-01, 1.3418e-01,\n",
      "         7.6037e-01, 3.4338e-01, 6.4892e-01, 7.5488e-01, 4.2756e-03, 7.6436e-01,\n",
      "         4.2191e-01, 3.3276e-01, 4.7655e-01, 5.2608e-01, 9.7714e-01, 4.9942e-01,\n",
      "         2.2480e-01, 4.9298e-01, 2.4920e-02, 8.9416e-01, 9.5874e-01, 8.3529e-01,\n",
      "         4.9427e-01, 5.3819e-01, 9.8376e-01, 6.6380e-01, 5.7941e-01, 3.4833e-01,\n",
      "         9.6085e-01, 4.0523e-01, 6.2140e-01, 7.5824e-01, 1.8271e-01, 7.5996e-01,\n",
      "         3.5588e-03, 4.0860e-01, 8.0547e-01, 7.1758e-01, 9.1424e-01, 5.6510e-01,\n",
      "         5.3623e-01, 2.0244e-01, 1.1920e-01, 1.7642e-01, 5.8818e-01, 6.0068e-01,\n",
      "         8.3077e-01, 1.5627e-01, 8.6420e-01, 3.8421e-01, 3.7568e-01, 2.7826e-01,\n",
      "         7.8656e-01, 9.4538e-01, 1.7626e-01, 4.1561e-01, 3.9558e-01, 8.6120e-01,\n",
      "         7.2800e-01, 8.8046e-01, 6.2512e-01, 6.7982e-01, 8.8229e-01, 7.2243e-01,\n",
      "         2.8907e-01, 7.0939e-01, 6.0554e-01, 1.3404e-01, 2.9499e-02, 4.6662e-01,\n",
      "         5.7168e-01, 6.9547e-01, 6.8620e-02, 9.9149e-01, 3.7686e-01, 6.9562e-01,\n",
      "         5.6652e-01, 5.1019e-01, 5.7671e-01, 5.6622e-01, 6.7818e-01, 9.9121e-01,\n",
      "         3.7523e-01, 4.2401e-01, 6.7420e-01, 8.9117e-01, 3.5496e-01, 1.1134e-03,\n",
      "         1.0946e-01, 7.4839e-02, 7.5141e-02, 1.1895e-01, 1.3417e-01, 7.5085e-01,\n",
      "         5.4471e-01, 4.6783e-01, 3.7472e-01, 4.9648e-01, 4.2796e-01, 7.2728e-01,\n",
      "         8.2539e-01, 7.2548e-01, 5.7811e-01, 3.8423e-02, 9.4442e-01, 5.8366e-01,\n",
      "         1.7953e-01, 7.0594e-01, 8.9062e-01, 7.9131e-01, 5.8848e-01, 6.5808e-01,\n",
      "         1.5543e-01, 5.6416e-01, 1.8816e-01, 6.8221e-01, 1.3162e-01, 9.5716e-01,\n",
      "         1.8978e-01, 6.8198e-01, 3.3690e-03, 7.4517e-01, 5.5137e-01, 3.3273e-01,\n",
      "         4.7786e-01, 6.6132e-01, 4.4151e-01, 9.8222e-02, 4.3858e-01, 8.1311e-01,\n",
      "         8.2417e-03, 9.6895e-01, 3.4755e-01, 5.7899e-01, 8.7581e-01, 7.1586e-01,\n",
      "         5.9025e-01, 6.2823e-02, 3.4453e-02, 1.4924e-01, 5.8647e-01, 5.0936e-01,\n",
      "         9.2249e-01, 5.5892e-01, 8.8368e-01, 1.4400e-02, 4.6129e-01, 2.5449e-01,\n",
      "         5.6388e-01, 7.0482e-01, 7.0419e-01, 3.3273e-01, 5.1374e-01, 9.2647e-01,\n",
      "         4.2539e-01, 5.4661e-01, 9.2730e-01, 2.0266e-01, 7.9009e-01, 4.5211e-01,\n",
      "         4.7490e-01, 4.3529e-01, 3.5673e-01, 3.1403e-01, 8.2485e-01, 2.5309e-01,\n",
      "         1.5886e-02, 7.0333e-01, 2.3195e-01, 3.3480e-01, 2.9497e-02, 6.9299e-01,\n",
      "         7.1897e-01, 4.1218e-01, 5.4332e-01, 9.6529e-01, 4.5583e-01, 4.5923e-01,\n",
      "         9.2122e-01, 8.4141e-02, 9.3350e-01, 6.9597e-01, 9.0299e-01, 2.5433e-01,\n",
      "         6.5271e-01, 5.4626e-01, 9.1072e-01, 3.8893e-01, 6.7510e-01, 5.0719e-01,\n",
      "         4.6582e-01, 4.1108e-01, 6.8564e-01, 1.9159e-01, 6.2015e-01, 8.6021e-01,\n",
      "         5.6010e-02, 4.7205e-01, 7.7652e-01, 4.5874e-01, 5.7350e-03, 5.4398e-01,\n",
      "         6.7493e-01, 5.7689e-01, 8.5026e-02, 2.8532e-01, 5.9306e-01, 7.7791e-02,\n",
      "         9.0972e-01, 9.5692e-01, 2.2005e-01, 4.0247e-02, 9.3417e-01, 2.5580e-01,\n",
      "         4.9033e-02, 1.7841e-01, 6.2465e-02, 5.5984e-01, 8.9721e-01, 4.9339e-01,\n",
      "         9.1152e-01, 5.1026e-01, 6.5208e-01, 5.4055e-02, 6.8599e-01, 3.0889e-01,\n",
      "         9.4986e-01, 8.2171e-01, 8.6588e-01, 7.3107e-01, 7.0960e-01, 2.3057e-01,\n",
      "         8.3226e-01, 7.9824e-01, 5.0581e-01, 8.4845e-01, 4.9578e-01, 1.9422e-01,\n",
      "         8.5509e-01, 5.4024e-01, 1.7815e-01, 2.8939e-01, 3.9243e-01, 6.1801e-01,\n",
      "         1.8681e-02, 2.2083e-01, 4.9503e-01, 2.3107e-01, 2.2720e-01, 9.0525e-01,\n",
      "         8.0453e-01, 8.1945e-01, 5.2414e-01, 5.3782e-01, 1.9753e-01, 1.6198e-02,\n",
      "         3.5935e-02, 1.1168e-03, 8.7025e-01, 2.7336e-01, 4.5369e-01, 1.2105e-01,\n",
      "         7.0224e-01, 3.9576e-01, 4.9326e-01, 1.8380e-01, 7.1577e-01, 3.8100e-01,\n",
      "         5.6190e-01, 1.9619e-01, 7.4161e-01, 7.2848e-01, 9.1219e-01, 8.2494e-01,\n",
      "         3.7837e-01, 8.7177e-01, 5.5211e-01, 7.6163e-01, 9.9487e-01, 5.4614e-01,\n",
      "         8.2103e-01, 8.5630e-01, 6.6703e-01, 5.3629e-01, 5.6143e-01, 2.4728e-02,\n",
      "         2.0071e-01, 1.4676e-01, 2.2855e-01, 3.9546e-01, 6.5392e-01, 4.1302e-01,\n",
      "         9.5415e-01, 1.2162e-01, 8.9594e-01, 6.6372e-01, 2.4964e-01, 8.6605e-01,\n",
      "         7.1819e-01, 3.2049e-01, 7.6702e-01, 7.0259e-02, 3.0663e-01, 9.0045e-01,\n",
      "         6.6955e-01, 2.4760e-01, 7.2653e-01, 3.2890e-01, 9.6135e-01, 6.2823e-01,\n",
      "         6.6985e-01, 4.1420e-01, 8.9296e-01, 8.0689e-01]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "print(data)\n",
    "print(\"============================================ \\n \\n\")\n",
    "labels = torch.rand(1, 1000)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass time (prediction )\n",
    "prediction = model(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
